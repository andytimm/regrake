---
title: Regularized Raking
subtitle: for Better Survey Estimates
format:
  clean-revealjs:
    self-contained: true
author:
  - name: Andy Timm
date: last-modified
bibliography: refs.bib
---

```{r, echo=F,output=F}
set.seed(605)
library(tidyverse)
library(survey)
library(ggrepel)
# Load regrake from parent directory (development package)
devtools::load_all("..")

```


## Introduction

**Raking** is among the most commonly used algorithms for building survey weights such that a unrepresentative sample can be used to make inferences about the general population.

**Regularized Raking** extends this framework,
allowing for more explicit and granular tradeoffs to be made on properties of the weights set.

### Goals:
1. Review vanilla raking, and clarify in what sense the weights it produces are 'optimal'.
2. Argue that vanilla raking's implicit objective isn't a great fit for modern survey inference,
and show why Regularized Raking is a good framework for improvements.

## Outline

-   Quick about me (& my weights!)
-   Raking: easy mode (introduce/review vanilla raking)
-   Raking: hard mode (when raking gets confusing)
-   Introduce regularized raking
-   Examples where RR really helps
-   RR in the survey weights cinematic universe (design vs. model based inference, comparisons to MRP, etc)

## About me (& my weights)

- Data scientist working in politics and advertising; both have given me a
lot of cause to think hard about weights.

Some recent types of weights I've worked on:

-  TV viewership weights (weight ~41M TV viewership panel to US TV viewing gen pop)
-  Political 'Microtargeting' Surveys (sample 5-10k people, weight to voter file, then model)
-  COVID Vaccine Message Testing (multi-arm RCT to convince people to get vaccine, weight to customers of our pharmacy partner)
-  \+ More (Market Research, RCTs in politics, RCTs in advertising with survey outcomes...)

# Raking (Easy Mode) {background-color="#40666e"}

## Let's introduce/review raking!

We have a **non-representative** sample from a larger universe, which we want
to use to make inferences about that universe. How do we best assign weights
to each observation in the sample to make (weighted) estimates representative?

In Easy Mode:

  - Our outcome $Y^N$ is correlated with demographics (even in easy mode)
  - We'll have **Ignorable Non-Response** (conditional on weighting vars)
  - More specifically, our sample happens to only need adjustment on *age* and
  *race* to match gen pop on this outcome
  - We somehow know everything we need to weight on to estimate $Y^N$
  - We also somehow know age and race proportions of the sample and universe
  completely without error

# Raking

Raking is an iterative algorithm for building sample weights such that (weighted)
sample marginal distributions match population marginal distributions.

We'll generate a synthetic universe, sample in biased fashion from it, then
rake it by hand and with `autumn` in R.

# Generating Data

```{r}
#| echo: true
n_universe <- 100000
n_sample <- 1000

# Universe Data Generating Process
universe <- tibble(
  age = sample(c("young", "old"),
               n_universe, replace = TRUE,
               prob = c(0.5, 0.5)),
  race = sample(c("white", "black", "asian", "hispanic", "other"),
                n_universe, replace = TRUE,
                prob = c(0.6, 0.1, 0.1, 0.1, 0.1)))

# Defining the outcome (probability of enjoying pineapple on pizza)
universe <- universe %>%
  mutate(enjoys_pineapple = plogis(rnorm(n_universe,-1,.5) +  #bias term
                              1*ifelse(race == "white", 1, 0) + # More likely if white
                              .5*ifelse(age == "old", 1, 0))) # More likely if older

```

## Check distribution

```{r}
#| echo: true
#| output: true

universe %>% summarize(mean_p = mean(enjoys_pineapple))

universe %>%
  group_by(age,race) %>%
  summarize(mean_p = mean(enjoys_pineapple)) %>%
  arrange(mean_p)
```
# Sample with Bias

```{r}
#| echo: true
#| output: true

# Parameters for the sample distribution
age_weights <- c(young = 1, old = 2)  # Old twice as likely
race_weights <- c(white = 5, black = 2,
                  asian = .5, hispanic = 1, other = 3)  # Also Wrong

universe <- universe %>%
  mutate(
    age_weight = age_weights[age],
    race_weight = race_weights[race],
    sample_weight = age_weight * race_weight
  )

# Draw the sample
sample_data <- universe %>%
  sample_n(size = n_sample, weight = sample_weight)

# Not our mean!
sample_data %>% summarize(mean_p = mean(enjoys_pineapple))
```

# Shake and Rake

Raking proceeds variable by variable, adding weights to fix bias on 1 marginal at a time,
until convergence.


```{r}
#| echo: true
#| output: true

# 1. Determine the Target Distribution for Age in the Universe
age_distribution_universe <- universe %>%
  count(age) %>%
  mutate(proportion = n / sum(n))

# 2. Calculate Initial Sample Weights (assuming equal probability of selection initially)
sample_data <- sample_data %>%
  mutate(weight = 1)


```

# Shake and Rake

```{r}
#| echo: true
#| output: true

# 3. Raking Step: Adjust Weights for Age
# a. Calculate the distribution of age in the sample
age_distribution_sample <- sample_data %>%
  count(age) %>%
  mutate(proportion = n / sum(n))

# b. Join with the target distribution to get the adjustment factor
adjustment_factors <- age_distribution_sample %>%
  left_join(age_distribution_universe, by = "age", suffix = c("_sample", "_universe")) %>%
  mutate(adjustment_factor = proportion_universe / proportion_sample)

# c. Apply the adjustment factor to the sample weights
sample_data <- sample_data %>%
  left_join(adjustment_factors, by = "age") %>%
  mutate(raked_weight = weight * adjustment_factor)

# 2x weight for older folks, as we'd expect
sample_data %>% group_by(age) %>% summarize(mean_weight = mean(raked_weight))
```
## Shake and Rake

### Now race

```{r}
# 1. Determine the Target Distribution for Race in the Universe
race_distribution_universe <- universe %>%
  count(race) %>%
  mutate(proportion = n / sum(n))

# 2. Raking Step for Race: Adjust Weights for Race
# a. Calculate the distribution of race in the sample with current weights
race_distribution_sample <- sample_data %>%
  count(race, wt = raked_weight) %>%
  mutate(proportion = n / sum(n))

# b. Join with the target distribution to get the race adjustment factor
race_adjustment_factors <- race_distribution_sample %>%
  left_join(race_distribution_universe, by = "race", suffix = c("_sample", "_universe")) %>%
  mutate(adjustment_factor_race = proportion_universe / proportion_sample)

# c. Apply the race adjustment factor to the already raked weights
sample_data <- sample_data %>%
  left_join(race_adjustment_factors, by = "race") %>%
  mutate(raked_weight = raked_weight * adjustment_factor_race)
```

```{r}
#| echo: true
#| output: true

# Not showing calculation, since they're identical.
sample_data %>% group_by(race) %>% summarize(mean_weight = mean(raked_weight))

# Notice: slightly further from universe than after first iter
sample_data %>% group_by(age) %>% summarize(mean_weight = mean(raked_weight))
```

`...` and repeat until sample means on all marginals match population ones.

## Less Manual: with `autumn`

`autumn` is a wonderful package for raking.

```{r}
#| echo: true
#| output: true

library(autumn)

target <- list(
  age = c(young = .5, old = .5),
  race = c(white = .6,
             hispanic = .1,
             black = .1,
             asian = .1,
             other = .1)
)
# See ?harvest for summary of other functionality
sample_data <- sample_data %>% harvest(target,max_weight = 10)

# With raked weights, we recover the population mean as expected
sample_data %>% summarize(mean_p = weighted.mean(enjoys_pineapple,weights))
```
# Raking (hard Mode) {background-color="#40666e"}

## Back to reality

Sadly, reality makes weighting much harder.

-   Phone response rates are now frequently less than 1%
-   Many survey researchers now use online or other non-probability sample methods
-   Outcomes are frequently related to propensity to respond in complicated ways
-   So, respondents are getting increasingly weird, in ways we can't just weight
out along a small set of census demographics

In other words, we live in a world of **Non-Ignorable Nonresponse**. What effects
does this have on weighting?

## A huge amount of pressure on weighting

:::: {.columns}

::: {.column width="50%"}
![](images/nyt_battleground_dims.png)
:::

::: {.column width="50%"}

- Trying to weight to cover all the issues
- On the left, everything the NYT weighted their recent 2024 battleground state
poll on.
- Of course, we probably believe interactions of variables matter too...
- Many of these variables are included because they seem correlated with
constructs we'd struggle to quantify directly
:::

::::

## Raking on everything isn't free

-   In many cases, raking on everything we want to won't even converge.
-   So we end up picking and choosing which variables and interactions theoretically
matter most.

Second class of issues: variance problems

-   Raking on more dimensions can lower bias, but it often adds variance too
-   This leads to heuristic ways to control variance
  - Rules of thumb like desiring a $deff_{kish}$ < 1.5
  - Trimming weights (cap/floor the tails of the weights distribution)

## Restating problems with raking on hard mode

Let's restate the last few slides in terms of specific problems alternative
weighting methods can solve:

1. "Making better weights" feels incredibly heuristic with vanilla raking
    i) How do we select variables and interactions?
    ii) How do we trade off bias for variance?
2. Variables are either "in" or "out"- there's no way to say e.g. "race x education
matters more to me than gender x race, but include both"
3. It's not clear in what sense the raking solution is optimal

# Regularized Raking {background-color="#40666e"}

## Regularized Raking

[@barratt_optimal_2021] show that the broader representative sample weights problem
can be understood as an optimization problem:

$$
\begin{array}{ll}
\operatorname{minimize} & \ell\left(f, f^{\mathrm{target}}\right)+\lambda r(w) \\
\text { subject to } & \quad w \geq 0, \quad \mathbf{1}^T w=1
\end{array}
$$
Let's step through this term by term.

## Regularized Raking - Loss

$$
\begin{array}{ll}
\operatorname{\color{grey}{minimize}} & \ell\left(f, f^{\mathrm{target}}\right)\color{grey}{+\lambda r(w)} \\
\color{grey}{ \text { subject to } } & \color{grey}{ \quad w \geq 0, \quad \mathbf{1}^T w=1}
\end{array}
$$

-   We have $f$ and $f^{target}$, which are functions of our sample and target population.
-   We'll also specify some loss function $\ell$, which clarifies how we want
$f$ and $f^{target}$ to be close

## Regularized Raking

### Loss examples - Ok, so what?

:::: {.columns}

::: {.column width="60%"}

$$
\ell\left(f, f^{\mathrm{des}}\right)= \begin{cases}0 & f = f^{target} \\ +\infty & \text { otherwise }\end{cases}
$$

-   In other words, $f$ must match $f^{target}$ exactly to be a valid solution
-   If $F$ is a collection of all the marginal expected values for our raking
dimensions, this is what vanilla raking requires!
:::

::: {.column width="40%"}

![](images/mcmahon_1.png)
:::

::::

## Regularized Raking

### Loss examples - Getting Interesting

:::: {.columns}

::: {.column width="60%"}

$$
\ell\left(f, f^{\mathrm{des}}\right)= \begin{cases}0 & f^{\min } \leq f \leq f^{\max } \\ +\infty & \text { otherwise }\end{cases}
$$

-   `...` But why do we always want exact matching on every raking variable?
-   That would expand the realm of possible solutions!

:::

::: {.column width="40%"}

![](images/mcmahon_2.png)
:::

::::

## Regularized Raking

### Loss examples - Now We're Getting Somewhere

:::: {.columns}

::: {.column width="40%"}

![](images/losses.png)

-   Continuous Loss is possible!
-   Closer can be better, and further away is not infinitely bad!

:::

::: {.column width="60%"}

![](images/mcmahon_3.png)
:::

::::

## Regularized Raking

### Loss examples - full force

:::: {.columns}

::: {.column width="60%"}

-   **We can have different losses for different $f$!**
-   Example:
    - require exact matching on single dimensions,
    - least squares on all interactions, and
    - different scaling such that different interactions are "worth" more according to theory or backtesting.


:::

::: {.column width="40%"}

![](images/mcmahon_4.png)
:::

::::

## Regularized Raking

### Regularization

$$
\begin{array}{ll}
\operatorname{\color{grey}{minimize}} & \color{grey}{\ell\left(f, f^{\mathrm{target}}\right)+}\lambda r(w) \\
\color{grey}{ \text { subject to } } & \color{grey}{ \quad w \geq 0, \quad \mathbf{1}^T w=1}
\end{array}
$$

::: {style="font-size: 85%;"}

-   We don't just care about adherence to population totals.
-   What about variance of weights, shape of distribution, etc?
-   We might want weights that are:
    - As uniform as possible
    - As close to some other target distribution with different variance properties
-   $r(w)$ can be chosen from a variety of regularizers that encode such preferences
- Also have a hyperparameter $\lambda$, which allows us to make explicit tradeoffs
between our loss term and our regularization term.

:::

## Regularized Raking

### Regularization Example 1

$$
r(w) = \sum_{i=1}^{n} w_i \operatorname{logw}_i
$$

-   This is the negative entropy, equivalent to the Kullbackâ€“Leibler divergence from the
weights distribution $w$ and the uniform distribution.
-   This would express a desire that our weights be as uniform as possible,
subject to all our other constraints
-   This is the other half of what vanilla raking does!

## Regularized Raking

### Regularization Example 2

$$
D_{\text{KL}}(w \parallel w^{des}) = \sum_{i=1}^{n} w \log\left(\frac{w}{w^{des}}\right)
$$

-   Alternatively, might want a weights distribution close to some target distribution
$w^{des}$, where closeness is defined in terms of KL divergence.
-   Motivations: minimizing extreme tails, more demand for smoothness

## Regularized Raking

### Constraints

$$
\begin{array}{ll}
\operatorname{\color{grey}{minimize}} & \color{grey}{\ell\left(f, f^{\mathrm{des}}\right)+\lambda r(w)} \\
\text { subject to } & \quad w \geq 0, \quad \mathbf{1}^T w=1
\end{array}
$$

-   Non-negative weights are useful (not all estimators play well with negative weights)
-   We want weights to sum to 1 (equivalently: rescale to sum to n)

::: aside
Quick note that there's a third constraint required $f = Fw$ that I'm
suppressing- it's necessary to state the optimization problem precisely, but
I don't want to go there this talk. See the paper for more detail.
:::

## So what does vanilla raking do?

$$
\ell\left(f, f^{\mathrm{target}}\right)=\left\{\begin{array}{ll}
0 & f=f^{\mathrm{target}}, \\
+\infty & \text { otherwise },
\end{array} \quad r(w)=\sum_{i=1}^n w_i \log w_i\right.
$$

-   Equality loss on all raking dimensions
-   Negative entropy regularizer
-   Hopefully, this gives some clarity about what raking optimizes for
-   As we've seen, this is far from the only choice of $\ell$ and $r(w)$; are
other choices better?

::: aside
If you're interested in a precise statement of this, raking can be seen as coordinate ascent on the dual of the maximum entropy problem.
See Appendix A of [@barratt_optimal_2021] for a illustration of this, or [@deville_generalized_1993;@teh_improving_2003].
:::

# Examples where RR helps {background-color="#40666e"}

## Data

::: {style="font-size: 85%;"}

Let's get back into some code, and comparisons. I'll be using a poll conducted by
Pew in 2016, with 2074 likely voter respondents. The outcome of interest is
**vote margin**, or the number of voters who favored Clinton minus those who
favored Trump.

I like this dataset for a few reasons:

-   Easy access
-   There is a correct answer, and we know it (Hilary won the national vote by 2.2%)
-   If you do not weight by education, you will be wrong by like 10 points :)
-   Decent variety of covariates

We'll treat estimates from the 2016 CCES as a target, because it's huge, representative,
and also easily accessible.

:::

```{r}

### Re-using and extending some example code from Caughey et al (2021)
# Check out their excellent book on target estimation and adjustment weighting
# https://codeocean.com/capsule/4173151/tree/v1
# To reproduce this analysis, you'll need to pull the CCES and pew data

# Convenience function that turns formulas into what's needed for calibrate
create_targets <- function (target_design, target_formula) {
    target_mf <- model.frame(target_formula, model.frame(target_design))
    target_mm <- model.matrix(target_formula, target_mf)
    wts <- weights(target_design)
    colSums(target_mm * wts) / sum(wts)
}

cces <- read_rds("data/cces.rds")
pew <- readRDS("data/pew.rds")


### Drop invalid cases
cces <- cces %>%
    filter((CC16_401 == "I definitely voted in the General Election.") &
           !is.na(commonweight_vv_post))

pew_design <- svydesign(ids = ~1, data = pew)

# survey's design object
cces_wt_design <- svydesign(ids = ~1, weights = ~commonweight_vv_post, data = cces)

vote_contrast <- quote((recode_vote_2016Democrat - recode_vote_2016Republican) /
                       (recode_vote_2016Democrat + recode_vote_2016Republican))
```

## Introducing `regrake`

Let's walk through a basic example in both `survey` (for vanilla raking) and
`regrake` (our R implementation of regularized raking based on [@barratt_optimal_2021]).
We'll weight just on age, gender, race, and region to show syntax.

### Vanilla raking with `survey`

```{r}
#| echo: true
#| output: true

formula_1 <- ~recode_age_bucket + recode_female + recode_race + recode_region

# Convenience function to make targets for a formula given a `survey` design obj
target_1 <- create_targets(cces_wt_design, formula_1)

pew_raked_1 <- calibrate(design = pew_design,
                       formula = formula_1,
                       population = target_1,
                       calfun = "raking")

svycontrast(svymean(~recode_vote_2016, pew_raked_1, na.rm = TRUE),
                        vote_contrast)
```


## Now with `regrake`

```{r}
#| echo: true
#| output: true

# With regrake, we directly specify what variables to match
# and use the CCES survey design as our population target
regrake_result_1 <- regrake(
  data = pew,
  formula = ~ rr_exact(recode_age_bucket) + rr_exact(recode_female) +
              rr_exact(recode_race) + rr_exact(recode_region),
  population_data = cces_wt_design,
  pop_type = "survey_design",
  regularizer = "entropy",
  lambda = 1
)
```

Steps:

1. Specify the formula with constraint types (`rr_exact()` for exact matching)
2. Choose a regularizer (entropy = prefer uniform weights)
3. Set the regularization strength ($\lambda$)

## Illustration: this is what raking is

```{r}
#| echo: true
#| output: true
n_pew <- nrow(pew)
regrake_weights_1 <- regrake_result_1$weights  # Already sums to n

regrake_design_1 <- svydesign(ids = ~1, data = pew, weights = regrake_weights_1,
                calibrate.formula = formula_1)

svycontrast(svymean(~recode_vote_2016, regrake_design_1, na.rm = TRUE),
                        vote_contrast)
```

Same result as vanilla raking! With entropy regularizer and $\lambda = 1$, we're
essentially doing the same optimization.

## Illustration: weight distributions

```{r}
#| echo: false
#| output: true
#| fig-height: 4.5
# Compare weight distributions (raking weights sum to 1, so multiply by n)
raked_1 <- enframe(weights(pew_raked_1) * n_pew) %>% mutate(method = "rake")
regrake_1 <- tibble(value = regrake_weights_1, method = "regrake")  # Already sums to n

compare_weights_1 <- bind_rows(raked_1, regrake_1)

compare_weights_1 %>%
  ggplot(aes(x = value, color = method, fill = method)) +
  geom_density(alpha=.5)+
  scale_fill_manual(values=c("red","blue")) +
  labs(x = "Weight", y = "Density", title = "Weight distributions: raking vs regrake")
```

Nearly identical weight distributions - as expected when we're solving the same problem.

## About as far as raking will take us

```{r}
#| echo: true
#| output: true

formula_2 <- ~ recode_age_bucket + recode_female +
    recode_region * recode_educ_3way * recode_race

# Convenience function to make targets for a formula given a `survey` design obj
target_2 <- create_targets(cces_wt_design,formula_2)

pew_raked_2 <- calibrate(design = pew_design,
                       formula = formula_2,
                       population = target_2,
                       calfun = "raking")

svycontrast(svymean(~recode_vote_2016, pew_raked_2, na.rm = TRUE),
                        vote_contrast)
```

# Something more complicated with regrake {background-color="#40666e"}

## Mixed loss types

With regularized raking, we can do something vanilla raking can't:
**exact matching on main effects, but soft (L2) matching on interactions**.

This lets us include more information without the convergence problems
that come from trying to exactly match every cell.

## Mixed loss types: the code

::: {style="font-size: 80%;"}

```{r}
#| echo: true
#| output: true

# Main effects: exact matching (rr_exact)
# Interactions: L2/soft matching (rr_l2)
regrake_result_2 <- regrake(
  data = pew,
  formula = ~ rr_exact(recode_age_bucket) + rr_exact(recode_female) +
              rr_exact(recode_inputstate) + rr_exact(recode_region) +
              rr_exact(recode_educ) + rr_exact(recode_race) +
              rr_l2(recode_region:recode_educ) +
              rr_l2(recode_region:recode_race) +
              rr_l2(recode_educ:recode_race) +
              rr_l2(recode_region:recode_educ:recode_race),
  population_data = cces_wt_design,
  pop_type = "survey_design",
  regularizer = "entropy",
  lambda = 10
)
```

:::

51 states + 6 main effect vars + all 2/3-way interactions = lots of constraints!

## Results: weight distributions

```{r}
#| echo: false
#| output: true
#| fig-height: 4.5
regrake_weights_2 <- regrake_result_2$weights  # Already sums to n

# Compare weight distributions (raking weights sum to 1, so multiply by n)
raked_2 <- enframe(weights(pew_raked_2) * n_pew) %>% mutate(method = "rake")
regrake_2 <- tibble(value = regrake_weights_2, method = "regrake")  # Already sums to n

compare_weights_2 <- bind_rows(raked_2, regrake_2)

compare_weights_2 %>%
  ggplot(aes(x = value, color = method, fill = method)) +
  geom_density(alpha=.5)+
  scale_fill_manual(values=c("red","blue")) +
  labs(x = "Weight", y = "Density",
       title = "Raking (exact on all) vs regrake (exact main + L2 interactions)")
```

regrake produces smoother weights - less extreme values.

## Results: vote margin estimate

```{r}
#| echo: true
#| output: true
regrake_design_2 <- svydesign(ids = ~1, data = pew, weights = regrake_weights_2,
                calibrate.formula = ~recode_age_bucket + recode_female +
     recode_region + recode_educ + recode_race)

svycontrast(svymean(~recode_vote_2016, regrake_design_2, na.rm = TRUE),
                        vote_contrast)
```

::: {style="font-size: 90%;"}

| Method | Vote Margin | True Margin |
|--------|-------------|-------------|
| Unweighted | ~5% | 2.2% |
| Vanilla raking (main + interactions) | 1.1% | 2.2% |
| **regrake (exact main + L2 interactions)** | **2.1%** | **2.2%** |

The soft constraints on interactions help us get closer to the truth!

:::

## Checking the balance: what did we actually achieve?

Unlike vanilla raking, soft constraints don't guarantee exact balance.
The `balance` data frame lets us inspect what regrake actually achieved:

```{r}
#| echo: false
#| output: true
#| fig-height: 4.5
# Get balance data and identify worst-balanced constraints
balance_df <- regrake_result_2$balance %>%
  mutate(
    constraint_type = factor(type, levels = c("exact", "l2")),
    abs_residual = abs(residual)
  )

# Find the 8 worst L2 constraints for labeling
worst_l2 <- balance_df %>%
  filter(type == "l2") %>%
  slice_max(abs_residual, n = 8)

balance_df %>%
  ggplot(aes(x = target, y = residual, color = constraint_type)) +
  geom_hline(yintercept = 0, linetype = "dashed", color = "gray50") +
  geom_point(alpha = 0.7) +
  geom_text_repel(
    data = worst_l2,
    aes(label = level),
    size = 2.5, max.overlaps = 10, color = "gray30"
  ) +
  scale_color_manual(
    values = c("exact" = "#2166AC", "l2" = "#B2182B"),
    labels = c("Exact (must match)", "L2 (soft)")
  ) +
  labs(
    x = "Target proportion",
    y = "Residual (achieved - target)",
    color = "Constraint type"
  ) +
  theme_minimal()
```

Exact constraints (blue) cluster at 0; L2 constraints (red) have slack.

# Regularized Raking in the Survey Weights Cinematic Universe {background-color="#40666e"}

##

## Versus other modern options

### MRP (and friends!)

**Multilevel Regression and Postratification** [@gelman_using_2019] use a regularized model to predict the outcome, and poststratify.

::: columns
::: {.column width="50%"}
#### Advantages

-   Fit directly on outcome(s) of interest- often more efficient
-   One set of weights regardless of outcome
-   Prior/training information can be used
:::

::: {.column width="50%"}
#### Disadvantages

-   Sometimes you actually want the whole posterior!
-   For most small area estimates, MRP is going to be more efficient
-   More generally, greater variety and flexibility of regularization
:::
:::

## Versus other modern options

### Multilevel Calibration

**Multilevel Calibration** [@ben-michael_multilevel_2023] also takes an optimization approach, requires exact matching on the marginals, more flexibility on interactions

::: columns
::: {.column width="50%"}
#### Advantages

-   RR comes with greater flexibility to define loss, regularization
-   RR optimizes for explicit objective (don't need to back out implicit prior)
:::

::: {.column width="50%"}
#### Disadvantages

-   MC has nice theory connecting estimator to outcome model
-   Implementation-wise, MC is probably more accessible as of today
:::
:::

## Regularized Raking in the Survey Weights Cinematic Universe

I've only been talking about only weighting today, so want to emphasize:

-   Good sampling matters (and more than how you weight)
-   Selection of weighting variables matters (and more than how you weight)

That said, how you weight is entangled with these, as more flexible weighting
gives you more options with the above.


## Thank you! {.smaller}

::: columns
::: {.column width="50%"}
#### Hire me!

-   Currently looking political/non-profit data science roles.
-   Experience as a data science manager, data scientist, and campaign staffer.
-   keywords: Causal Inference, Bayes, HTE estimation, Surveys
:::

::: {.column width="50%"}
#### Materials/Suggestions

-   [@barratt_optimal_2021] for finer details of optimization behind talk today
-   For perspective on challenges of modern survey weighting, highly recommend **A New Paradigm for Polling** [@bailey_new_2023]
-   For how to think about picking targets and estimating them, I adore [@caughey_target_2020]- **Target Estimation and Adjustment Weighting for Survey Nonresponse and Sampling Bias**
:::
:::

